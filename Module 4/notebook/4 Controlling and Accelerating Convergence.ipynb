{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../PythonScripts\") ## enter the path where you placed any scripts you'd like to load\n",
    "from prereqs import * ## imports wrapper functions such as plot, hist, and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Introduction \n",
    "\n",
    "\n",
    "\n",
    "1. Monitoring Convergence using multiple MC experiments\n",
    "\n",
    "2. Antithetic Variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Monitoring Convergence  \n",
    "\n",
    "As a toy example, consider the simple function $h(x) = [cos(50x) + sine(50x)]^2$. Using a simple MC algorithm, we can estimate $\\theta = \\int_0^1 h(x)$. Let us generate $n$ samples $x_1, ..., x_n \\sim Unif(0,1)$, such that $\\displaystyle \\theta = E[h(x)] \\approx \\frac{1}{n} \\sum^n h(x_i)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(456789) ## set seed\n",
    "n = 10000\n",
    "h = lambda x: ( np.cos(50*x) + np.sin(50*x) )**2\n",
    "u = np.random.rand(n)\n",
    "\n",
    "theta_est = cumMean(h(u))\n",
    "se = cumSE(h(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, n+1)\n",
    "plt.plot(x, theta_est + 1.96*se, color = '0.4') ## can also use plt.hlines function\n",
    "plt.plot(x, theta_est - 1.96*se, color = '0.4')\n",
    "plot(x, y = theta_est, show=False) ## don't automatically plot\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.title(r\"Estimates of $\\theta$\")\n",
    "plt.show()\n",
    "u = np.random.rand(n)\n",
    "\n",
    "theta_est = cumMean(h(u))\n",
    "se = cumSE(h(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = np.random.rand(n) ## new experiment\n",
    "\n",
    "theta_est2 = cumMean(h(u2))  ## new experiment cumulative mean\n",
    "\n",
    "x = range(1, n+1)\n",
    "plot(x, y = theta_est, show=False) ## don't automatically plot\n",
    "plot(x, y = theta_est + 1.96*se, show=False, color = '0.4') ## can also use plt.hlines function\n",
    "plot(x, y = theta_est - 1.96*se, show=False, color = '0.4')\n",
    "plot(x, y = cumMean(h(u2)), show=False)\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.title(r\"Estimates of $\\theta$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we now add a new experiment, the new sample is not guaranteed to stay within the 95% CI.\n",
    "\n",
    "\n",
    "## Monitoring Convergence  \n",
    "\n",
    "\n",
    "Monitoring convergence of Monte Carlo samples is important to assessing the quality of estimators. For some MC estimate $\\theta_{MC}$, it is possible to run many parallel processes and graphically monitor how they converge, and from those samples obtain a confidence band. \n",
    "\n",
    "However, this may be computationally costly, and resource (e.g. hardware + time) intensive.\n",
    "\n",
    "To understand how much memory our experiments are using, we can use the library [memory_profiler](https://pypi.python.org/pypi/memory_profiler). Many programming langugages have libraries for profiling code, and monitoring bottlenecks and resource consumption.\n",
    "\n",
    "- To install: `pip install memory_profiler`\n",
    "- Or in Anaconda: `conda install -c chroxvi memory_profiler` \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### IPython integration\n",
    "\n",
    "After installing the module, if you use IPython, you can use the `%mprun`, `%%mprun`, `%memit` and `%%memit` magics.\n",
    "\n",
    "For IPython 0.11+, you can use the module directly as an extension, with %load_ext memory_profiler\n",
    "\n",
    "To activate it whenever you start IPython, edit the configuration file for your IPython profile, ~/.ipython/profile_default/ipython_config.py, to register the extension like this (If you already have other extensions, just add this one to the list):\n",
    "\n",
    "    c.InteractiveShellApp.extensions = [\n",
    "        'memory_profiler',\n",
    "    ]\n",
    "\n",
    "\n",
    "If the config file doesnâ€™t already exist, run the command `ipython profile create` in a terminal (Windows CMD terminal, Linux terminal, or iOS terminal).\n",
    "\n",
    "\n",
    "\n",
    "Then you should be able to run something simple like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Monitoring Convergence  \n",
    "\n",
    "While running, monitor your computer's resource manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parallel monte carlo samples\n",
    "M = 200\n",
    "n = 10000\n",
    "X = np.random.uniform(size = (n, M))\n",
    "h_samples = h(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit X = np.random.uniform(size = (n, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we'll need to apply cums down columns,\n",
    "## so remember that:\n",
    "beta = np.array([[1,2,3], [1,2,3], [1,2,3]])\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(beta, axis = 0) ## sums down columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(beta, axis = 1) ## sums rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cummean(matrix, axis = 0):\n",
    "    n = matrix.shape[0]\n",
    "    denom = np.array(range(1, n+1)).reshape(n, 1)\n",
    "    return np.cumsum(matrix, axis = axis) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thetaEstimates = np.apply_along_axis(arr=h_samples, func1d = cumMean, axis = 1)\n",
    "thetaEstimates = cummean(h_samples, axis = 0)\n",
    "\n",
    "quantiles = lambda x: np.percentile(a=x, q=[2.5, 97.5]) ## get 2.5%, and 97.5% for Confidence Interval\n",
    "\n",
    "parallelCI = np.percentile(thetaEstimates, q = [2.5, 97.5], axis = 1)\n",
    "print(parallelCI.shape) ## return 2 rows, 10000 columns...numpy is strange!\n",
    "\n",
    "print(np.mean(parallelCI[1])) ## upper bounds\n",
    "print(np.mean(parallelCI[0]))  ## lower bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit thetaEstimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelCI[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, n+1)\n",
    "fig, ax = plt.subplots() ## needed to add polygon\n",
    "plot(x, parallelCI[0], color = '0.4', show=False)\n",
    "plot(x, parallelCI[1], color = '0.4', show=False)\n",
    "plot(x, theta_est, show=False)\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.title(r\"$\\theta$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Monitoring Convergence  \n",
    "\n",
    "An approximate but cheaper version of this basic Monte Carlo estimate of the variability is to bootstrap the originally obtained samples and from there estimate a 95% confidence band.\n",
    "\n",
    "\n",
    "\n",
    "## Monitoring Convergence | Bootstrap CI  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap\n",
    "## parallel monte carlo samples\n",
    "M = 200\n",
    "n = 10000\n",
    "ind = np.random.choice(a = range(n), size = n*M, replace=True) ## sample with replacement!\n",
    "bootsamples = h(X[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootestimates = cummean(h_samples, axis = 0)\n",
    "\n",
    "bootquantiles = lambda x: np.percentile(a=x, q=[2.5, 97.5]) ## get 2.5%, and 97.5% for Confidence Interval\n",
    "\n",
    "bootCI = np.percentile(thetaEstimates, q = [2.5, 97.5], axis = 1)\n",
    "print(bootCI.shape) ## return 2 rows, 10000 columns...numpy is strange!\n",
    "\n",
    "print(np.mean(bootCI[1])) ## upper bounds\n",
    "print(np.mean(bootCI[0]))  ## lower bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, n+1)\n",
    "fig, ax = plt.subplots() ## needed to add polygon\n",
    "plot(x, bootCI[0], show=False)\n",
    "plot(x, bootCI[1], show=False)\n",
    "plot(x, theta_est, show=False)\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.title(r\"$\\theta$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Antithetic Variables   \n",
    "\n",
    "In previous experiments, when we've worked to generate pseudo-random samples from distributions, we've worked with *iid* (independent and identically distributed) peuso-random samples from an instrumental distribution. \n",
    "\n",
    "Generally, *iid* samples are always preferable, but not always cost efficient. As problems become more complicated, generating random samples from a target distribution will become more cumbersome and time/resource consuming. \n",
    "\n",
    "Therefore, in this section we will present methods in which we can double down on our generated samples to speed up convergence and utilize more of our available resources.\n",
    "\n",
    "\n",
    "## Antithetic Variables   \n",
    "\n",
    "Suppose we are interested in estimating a parameter $\\theta$ and we have two unbiased estimators $X$ and $Y$ with finite variances $\\sigma^2_X = \\sigma^2_Y = \\sigma^2$. The average of the two unbiased estimators is also an unbiased estimator, $Z = (X + Y) / 2$. Given $X, Y$ and $Z$ are all unbiased, we can compare them using their variances and choose the smallest of the three. \n",
    "\n",
    "The variance for $Z$ is given by \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(Z) & =  Var(\\frac{X + Y}{2})  = \\left(\\frac{1}{2} \\right)^2 \\times Var(X + Y) \\\\\n",
    "\\ & = \\frac{1}{4} \\left(  Var(X) + Var(Y) + 2 Cov(X, Y) \\right) \\\\\n",
    "\\ & = \\frac{1}{4} Var(X) + \\frac{1}{4} Var(Y) + \\frac{1}{2} Cov(X, Y) \\\\\n",
    "\\ & = \\frac{1}{4} \\sigma^2 + \\frac{1}{4} \\sigma^2 + \\frac{1}{2} Cov(X, Y) \\\\\n",
    "\\ & = \\frac{1}{2} (\\sigma^2 +  Cov(X, Y))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $Cov(X, Y) < 0$ (or $Cor(X, Y) < 0$), then $Var(Z) < \\sigma^2$, therefore, $Z$ would perform better as an unbiased estimator. In this scenario, $X$ and $Y$ would be *antithetic* to each other; thus they are *antithetic variables*.\n",
    "\n",
    "\n",
    "## Antithetic Variables   \n",
    "\n",
    "The Monte Carlo integration estimator\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta & = \\displaystyle \\int_{- \\infty}^{\\infty} h(x) f(x) dx \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Let $X$ and $Y$ denote random samples, $x_1, ..., x_n$ and $y_1, ..., y_n$, respectively. If $X$ and $Y$ are negatively correlated, then the estimator $\\hat{\\theta}$ of $\\theta$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\theta} & = \\displaystyle \\frac{1}{2n} \\sum_{i = 1}^{n} [h(x_i) + h(y_i)] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "is more efficient than the estimator $\\displaystyle \\hat{\\theta} = \\frac{1}{2n} \\sum_{i = 1}^{2n} h(x_i)$. The random variables $X$ and $Y$ are then called *antithetic variables*. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Antithetic Variables   \n",
    "\n",
    "Albeit useful, this method is not always possible. For arbitrary transformations $h(.)$, it is not always possible to generate negatively correlations $X$ and $Y$.\n",
    "\n",
    "As covered in the introduction, we can generate negatively correlated samples from a uniform distribution.\n",
    "\n",
    "Consider the following. Let $h(x) = e^x$, where we'd like to solve $\\int_0^1 h(x) dx$.\n",
    "We can take this problem as\n",
    "\n",
    "$$\n",
    "\\theta = \\int_0^1 h(x) dx = \\int_0^1 h(x) f(x) dx\n",
    "$$\n",
    "where $f(x) = 1$ for a $Unif(0,1)$ distribution. Then\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "u = np.random.rand(n)\n",
    "\n",
    "h = lambda x: np.exp(x)\n",
    "\n",
    "thetaHat = np.mean(h(u))\n",
    "print(thetaHat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.integrate import quad\n",
    "print(quad(h, 0, 1)) ## return estimate and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## using antithetic variables\n",
    "\n",
    "u1 = np.random.uniform(size = int(n/2))\n",
    "u2 = 1 - u1\n",
    "\n",
    "thetaHatAV = np.mean([h(u1), h(u2)])\n",
    "print(thetaHatAV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.pearsonr(x=h(u1), y = h(u2)) ## correlation nearly -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise   \n",
    "\n",
    "Suppose $X \\sim N(0,1)$ and we wish to estimate $\\theta = E[h(X)]$ where $h(X) = \\frac{x}{(2^x - 1)}$. \n",
    "\n",
    "By regular Monte Carlo estimation, we can estimate $\\theta$ with $n = 10^6$ samples from $N(0,1)$. \n",
    "\n",
    "By antithetic variable estimation we can estimate $\\theta$ by $m = n/2 = 50,000$. Construct an antithetic sample where $Cor(-X, X) = -1$. \n",
    "\n",
    "Compare your results with regular MC estimation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Appendix   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**6\n",
    "m = int(n/2)\n",
    "\n",
    "h = lambda x: x/(2**x - 1)  ## anonymous function\n",
    "\n",
    "y = np.random.normal(size = n)\n",
    "theta_MC = np.mean(h(y))\n",
    "\n",
    "w = np.random.normal(size = m)\n",
    "theta_AS = np.sum([h(w) + h(-1 * w)]) / n\n",
    "\n",
    "print(theta_MC)\n",
    "print(theta_AS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard errors\n",
    "rho,_ = stats.pearsonr(x = h(w), y = h(-w))\n",
    "seAnti = (1+rho)*np.var(h(w))/n\n",
    "\n",
    "print(rho)\n",
    "print(seAnti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
