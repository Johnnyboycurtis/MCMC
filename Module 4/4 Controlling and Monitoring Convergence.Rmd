---
title: "Monitoring and Accelerating Convergence"
author: "Jonathan Navarrete"
date: "  "
output:   
  ioslides_presentation:
    theme: simplex
    smaller: true
    wide: true
    css: slides.css
    
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 95
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


## Introduction { .selectable }



1. Monitoring Convergence using multiple MC experiments

2. Antithetic Variables




## Monitoring Convergence  { .selectable }

As a toy example, consider the simple function $h(x) = [cos(50x) + sine(50x)]^2$. Using a simple MC algorithm, we can estimate $\theta = \int_0^1 h(x)$. Let us generate $n$ samples $x_1, ..., x_n \sim Unif(0,1)$, such that $\displaystyle \theta = E[h(x)] \approx \frac{1}{n} \sum^n h(x_i)$. 

```{r, fig.align='center', fig.height=3, fig.width=6}
set.seed(3456)
n = 80000
x = runif(n)
h = function(x){
  v = (cos(50*x) + sin(50*x))^2
  return(v)
}

#thetaHat = mean(h(x)) # theta hat

theta_est = cumsum(h(x))/1:n ## cumulative mean

se = sqrt( cumsum((h(x) - theta_est)^2 ) / 1:n ) / sqrt(1:n)
```


## Monitoring Convergence  { .selectable }


```{r, fig.height=3.7}
par(pin = c(5,3))
plot(x = 1:n, y = theta_est, type = "l", lty=2, ylim = c(0.7, 1.2), xlim = c(0, 10000))
lines(x = 1:n, y = theta_est - 1.96*se, col = "blue")
lines(x = 1:n, y = theta_est + 1.96*se, col = "blue")
legend(x = 6000, y = 0.85, legend = c(expression(hat(theta)), "CI"), 
       border = "white", col = c("black", "blue"), lty = c(2, 1))

```



## Monitoring Convergence  { .selectable }

If we now add a new experiment, the new sample is not guaranteed to stay within the 95% CI.

```{r, fig.height=3.7, echo = FALSE}
par(pin = c(5,3))
plot(x = 1:n, y = theta_est, type = "l", lty=2, ylim = c(0.7, 1.2), xlim = c(0, 10000))
lines(x = 1:n, y = theta_est - 1.96*se, col = "blue")
lines(x = 1:n, y = theta_est + 1.96*se, col = "blue")
z = runif(n) ## repeat the experiment
lines(x = 1:n, y = cumsum(h(z))/1:n, col = "red", lty = 3)

legend(x = 6000, y = 0.85,
       legend = c(expression(hat(theta)), "CI", "New Experiment"), 
       border = "white", col = c("black", "blue", "red"), lty = c(2, 1, 3))

```




## Monitoring Convergence  { .selectable }


Monitoring convergence of Monte Carlo samples is important to assessing the quality of estimators. For some MC estimate $\theta_{MC}$, it is possible to run many parallel processes and graphically monitor how they converge, and from those samples obtain a confidence band. 

However, this may be computationally costly, and resource (e.g. hardware + time) intensive.

To understand how much memory our experiments are using, we can use the library [`profmem`](https://cran.r-project.org/web/packages/profmem/vignettes/profmem.html). Many programming langugages have libraries for profiling code, and monitoring bottlenecks and resource consumption.

- To install: `install.packages("profmem")`


## Monitoring Convergence  { .selectable }

While running, monitor your computer's resource manager

```{r}
## parallel monte carlo samples
M = 200L
X = matrix(data = runif(n*M), nrow = n, ncol = M)
h_samples = h(X)
thetaEstimates = apply(X = h_samples, MARGIN = 2, FUN = function(v){ cumsum(v)/1:n } )

parallelCI = t(apply(X = thetaEstimates, MARGIN = 1, FUN = quantile, c(0.025, 0.50, 0.975)))

summary(parallelCI)

integrate(f = h, lower = 0, upper = 1) ## comparison
```


## Monitoring Convergence | Parallel CI { .selectable }

```{r, echo=FALSE}
par(pin = c(5,3))
plot(x = 1:n, y = theta_est, type = "l", lty=2, ylim = c(0.7, 1.2), xlim = c(0, 10000))
polygon(x = c(1:n, rev(1:n)), y = c(parallelCI[,1], rev(parallelCI[,3])), lty = 4, border = "gray30", col = "gray90")
lines(x = 1:n, y = cumsum(h(z))/1:n, col = "red", lty = 3)

legend(x = 30000, y = 0.85, 
       legend = c(expression(hat(theta)), "CI", "New Experiment"), 
       border = "white", col = c("black", "blue", "red"), lty = c(2, 1, 3))

```



## Monitoring Convergence  { .selectable }

An approximate but cheaper version of this basic Monte Carlo estimate of the variability is to bootstrap the originally obtained samples and from there estimate a 95% confidence band.

```{r}
## bootstrap
M = 200L
ind = sample(x = 1:n, size = n*M, replace = TRUE) ## sample indices
boot_samples = matrix(data = h(x[ind]), nrow = n, ncol = M) ## matrix of samples
boot_est = apply(X = boot_samples, MARGIN = 2, FUN = cumsum) / 1:n ## matrix

bootCI = t(apply(X = boot_est, MARGIN = 1, FUN = quantile, c(0.025, 0.50, 0.975)))
summary(bootCI)
```



## Monitoring Convergence | Bootstrap CI  { .selectable }


```{r, fig.height=3.7, echo = FALSE}
par(pin = c(5,3))
plot(x = 1:n, y = theta_est, type = "l", lty=2, ylim = c(0.7, 1.2), xlim = c(0, 10000))
polygon(x = c(1:n, rev(1:n)), y = c(bootCI[,1], rev(bootCI[,3])), lty = 4, border = "gray30", col = "gray80")
lines(x = 1:n, y = cumsum(h(z))/1:n, col = "red", lty = 3)

legend(x = 30000, y = 0.85, 
       legend = c(expression(hat(theta)), "CI", "New Experiment"), 
       border = "white", col = c("black", "blue", "red"), lty = c(2, 1, 3))

```







## Antithetic Variables   { .selectable }

In previous experiments, when we've worked to generate pseudo-random samples from distributions, we've worked with *iid* (independent and identically distributed) peuso-random samples from an instrumental distribution. 

Generally, *iid* samples are always preferable, but not always cost efficient. As problems become more complicated, generating random samples from a target distribution will become more cumbersome and time/resource consuming. 

Therefore, in this section we will present methods in which we can double down on our generated samples to speed up convergence and utilize more of our available resources.


## Antithetic Variables   { .selectable }

Suppose we are interested in estimating a parameter $\theta$ and we have two unbiased estimators $X$ and $Y$ with finite variances $\sigma^2_X = \sigma^2_Y = \sigma^2$. The average of the two unbiased estimators is also an unbiased estimator, $Z = (X + Y) / 2$. Given $X, Y$ and $Z$ are all unbiased, we can compare them using their variances and choose the smallest of the three. 

The variance for $Z$ is given by 
$$
\begin{aligned}
Var(Z) & =  Var(\frac{X + Y}{2})  = \left(\frac{1}{2} \right)^2 \times Var(X + Y) \\
\ & = \frac{1}{4} \left(  Var(X) + Var(Y) + 2 Cov(X, Y) \right) \\
\ & = \frac{1}{4} Var(X) + \frac{1}{4} Var(Y) + \frac{1}{2} Cov(X, Y) \\
\ & = \frac{1}{4} \sigma^2 + \frac{1}{4} \sigma^2 + \frac{1}{2} Cov(X, Y) \\
\ & = \frac{1}{2} (\sigma^2 +  Cov(X, Y))
\end{aligned}
$$

If $Cov(X, Y) < 0$ (or $Cor(X, Y) < 0$), then $Var(Z) < \sigma^2$, therefore, $Z$ would perform better as an unbiased estimator. In this scenario, $X$ and $Y$ would be *antithetic* to each other; thus they are *antithetic variables*.


## Antithetic Variables   { .selectable }

The Monte Carlo integration estimator
$$
\begin{aligned}
\theta & = \displaystyle \int_{- \infty}^{\infty} h(x) f(x) dx \\
\end{aligned}
$$
Let $X$ and $Y$ denote random samples, $x_1, ..., x_n$ and $y_1, ..., y_n$, respectively. If $X$ and $Y$ are negatively correlated, then the estimator $\hat{\theta}$ of $\theta$
$$
\begin{aligned}
\hat{\theta} & = \displaystyle \frac{1}{2n} \sum_{i = 1}^{n} [h(x_i) + h(y_i)] \\
\end{aligned}
$$
is more efficient than the estimator $\displaystyle \hat{\theta} = \frac{1}{2n} \sum_{i = 1}^{2n} h(x_i)$. The random variables $X$ and $Y$ are then called *antithetic variables*. 




## Antithetic Variables   { .selectable }

Albeit useful, this method is not always possible. For arbitrary transformations $h(.)$, it is not always possible to generate negatively correlations $X$ and $Y$.

As covered in the introduction, we can generate negatively correlated samples from a uniform distribution.

Consider the following. Let $h(x) = e^x$, where we'd like to solve $\int_0^1 h(x) dx$.
We can take this problem as

$$
\theta = \int_0^1 h(x) dx = \int_0^1 h(x) f(x) dx
$$
where $f(x) = 1$ for a $Unif(0,1)$ distribution. Then


## Antithetic Variables   { .selectable }

```{r}
n = 50000
u = runif(n)

h = function(x){exp(x)}

thetaHat = mean(h(u))
thetaHat
integrate(f = h, lower = 0, upper = 1)
```



## Antithetic Variables   { .selectable }

```{r}
u1 = runif(n/2)
u2 = 1 - u1
h = function(x){exp(x)}

thetaHatAV = mean(c(h(u1), h(u2)))
thetaHatAV

cor(x = h(u1), y = h(u2)) ## correlation nearly -1
```










## Exercise   { .selectable }

Suppose $X \sim N(0,1)$ and we wish to estimate $\theta = E[h(X)]$ where $h(X) = \frac{x}{(2^x - 1)}$. 

By regular Monte Carlo estimation, we can estimate $\theta$ with $n = 10^6$ samples from $N(0,1)$. 

By antithetic variable estimation we can estimate $\theta$ by $m = n/2 = 50,000$. Construct an antithetic sample where $Cor(-X, X) = -1$. 

Compare your results with regular MC estimation.





# Appendix   { .selectable }



## Solution to Exercise  { .selectable }

```{r}

n = 10^6
m = n/2

h <- function(x){ x/(2^x - 1) }

y = rnorm(n)
theta_MC = mean(h(y))

w = rnorm(m)
theta_AS = sum(h(w) + h(-1 * w)) / n

print(theta_MC)
print(theta_AS)
```



## Solution to Exercise  { .selectable }


```{r}
## standard errors
rho = cor(h(w),h(-w))
se.a = (1+rho)*var(h(w))/n

print(rho)
print(se.a)

```





