---
title: "Generating Random Variables"
author: "Jonathan Navarrete"
date: "August 5, 2017"
output: 
  ioslides_presentation:
    theme: simple
    smaller: true
    wide: true
    
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 95
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



## Introduction

In this section we'll cover the following methods for generating random numbers from a target distribution.

1. Inverse Transform Method

2. Accept-Reject Method

3. Transformation Method

4. Sums and Mixture distributions


## Pseu-random numbers

One of the fundamental tools required in Monte Carlo methods is the ability to generate consistent *pseudo*-random variables from a specified probability distribution. 

- One of the most fundamental number generation is the uniform random number generation

- Can sample from many distributions with the help of uniform random numbers

- Pseudo random number genrators rely on the assumption that computational methods can consistently generate uniform random numbers.

In R, we can control how random variables are generated. For example, we can specify seed for random number generation, see the function [`set.seed`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html)

- `set.seed` allows us to control the start and flow (*state*) of random number generation with a numeric "seed"

- Useful for reproducibility

- On StackOverflow: [Reasons for using the set.seed function](https://stackoverflow.com/questions/13605271/reasons-for-using-the-set-seed-function)


## Generation uniform samples

Random number generation heavily relies on being able to obtain reliable samples from a Uniform distribution. There is a long history of work that has gone into this. Computers on their own can't generate *randomness*, but they can simulate what *random* may look like.


```{r, eval = FALSE}
u = runif(2000)

hist(u, probability=TRUE)
lines(density(u), xlim=c(0,1))

## Q-Q plot for `runif` data against true theoretical distribution:
qqplot(qunif(ppoints(500)), u,  main = expression("Q-Q plot for Unif(0,1)"))
qqline(u, distribution = qunif, prob = c(0.1, 0.6), col = 2)

```



```{r, fig.height=2, fig.width=6, echo=FALSE}
u = runif(2000)

par(pin = c(2,1), mfrow = c(1,2))
hist(u, probability=TRUE)
lines(density(u), xlim=c(0,1))

## Q-Q plot for `runif` data against true theoretical distribution:
qqplot(qunif(ppoints(500)), u,  main = expression("Q-Q plot for Unif(0,1)"))
qqline(u, distribution = qunif, prob = c(0.1, 0.6), col = 2)


```


## Generation uniform samples

If $u \sim Unif(0,1)$, then $(1 - u) \sim Unif(0,1)$

```{r, fig.height=3.2, fig.width=6, echo=FALSE}
require(graphics)
palette(value = rainbow(6))

par(pin = c(2,2), mfrow = c(1,2))
qqplot(qunif(ppoints(500)),1-u, main = "U against 1 - U")
qqline(u, distribution = qunif, col = 2)

acf(u, main = "autocorrelation of samples") 


```


```{r, eval = FALSE}
qqplot(qunif(ppoints(500)),1-u, main = "U against 1 - U")
qqline(u, distribution = qunif, col = 2)

acf(u, main = "autocorrelation of samples") 

```

It is important to generate samples that are not correlated with each other!



## Generation uniform samples, pt. 2

Once we can simulate $u \sim Unif(0,1)$, we can begin generating samples from other target distributions. How could we simulate $v \sim Unif(0,10)$? Well, we could simply include a multiplicative constant such that $v = 10 \times u$.

```{r, eval=FALSE}
v = 10 * runif(1000, 0, 1) ## v ~ unif(0, 10)
hist(v)

w = runif(1000, 0, 10) ## unif(0, 10)
hist(w)


```


```{r, fig.height=3.2, fig.width=6, echo=FALSE}
par(mfrow = c(1,2))
v = 10 * runif(1000, 0, 1) ## v ~ unif(0, 10)
hist(v)

w = runif(1000, 0, 10) ## unif(0, 10)
hist(w)


```








## Inverse Transform Method

General idea: only using a uniform distribution, generate random values and use an inverse CDF of the target distribution for which you wish to simulate. See the following link for further discussion: [How does the inverse transform method work?][1]

### Theorem (Probability Integral Transformation): 
If $X$ is a continuous random variable with CDF $F_X(X)$, then $U = F_X(X) \sim Unif(0,1)$. If $U \sim Unif(0,1)$, then for all $x \in \mathbb R$

$$
\begin{aligned}
P(F_X^{-1}(U) \leq x) & = P(\inf\{t: F_X(t) = U\} \leq x)\\
\\ & = P(U \leq F_X(x))\\
\\ & =F_U(F_X(x)) \\
\\ & = F_X(x) = P(X \leq x)
\end{aligned}
$$
and therefore $F_X^{-1}(U)$ has the same distribution as $X$.




## Inverse Transform Method, pt. 2


Steps:

1. For target probability density function (*pdf*) $f(x)$, calculate the inverse of the CDF by setting $F(x) = U$, then solving for $X$, for which $U \sim Unif(0,1)$.

2. Generate N random numbers from $U \sim Unif(0,1)$

3. Plug in $U$ observed values in $F^{-1}(U)$ to obtain $n$ $x$ values for which $X \sim f(x)$



## Example: Exponential distribution

Suppose we are interested in generating $n = 10,000$ random values from an Exponential distribution 

1. $f(X) = \lambda e^{- \lambda X}$

2. $F(X) = 1 - e^{- \lambda X} = U$

3. $F^{-1}(U) = - 1/\lambda \ log(1 - U)$; can use $(1-u)$ or $u$, since both are uniformly distributed.

If we set $\lambda = 5$, then


```{r cars}
N = 10^4
u = runif(N)

fInv = function(u){
  (-1/5) * log(u) ## or log(1-u)
}

outSamples = fInv(u)

```

## Example: Exponential distribution, pt. 2


```{r, eval=FALSE}
hist(outSamples, probability = TRUE, main = "Inv Trans")
lines(x = ppoints(200), y = dexp(x = ppoints(200), rate = 5), 
      col = "blue")
hist(rexp(n = N, rate = 5), probability = TRUE, main = "rexp")
lines(x = ppoints(200), y = dexp(x = ppoints(200), rate = 5), 
      col = "blue")

```


```{r, fig.height=3, fig.width=6, echo=FALSE}
## not for show
## echo = FALSE
par(mfrow = c(1,2), mar = c(3, 4, 1, 2))
hist(outSamples, probability = TRUE, main = "Inv Trans")
lines(x = ppoints(200), y = dexp(x = ppoints(200), rate = 5), 
      col = "blue")
hist(rexp(n = N, rate = 5), probability = TRUE, main = "rexp")
lines(x = ppoints(200), y = dexp(x = ppoints(200), rate = 5), 
      col = "blue")
```



## Example: Pareto Distribution

For information on the Pareto distribution, please see: [Pareto Distribution][3]

The $Pareto(a,b)$ distribution has CDF $F(X \leq x) = 1 - (\frac{b}{x})^a$ for $x \geq b > 0, \ a > 0$


1. First set $F(x) = U$, where $U \sim Unif(0,1)$, then solve for $X$
$$
\displaystyle
\begin{aligned}
1 - \left( \frac{b}{x} \right)^2 & = U \\
\ \left(\frac{b}{x} \right)^a & = 1 - U \\
\  \frac{b}{x} & = (1 - U)^{1/a} \\
\ x & = b \times (1 - U)^{-1/a} \\
\ & = F_X^{-1}(U) \\
\end{aligned}
$$



## Example: Pareto Distribution, pt. 2


```{r}
set.seed(123)
n = 1000
U =runif(n)
a = 3
b = 2
X = b*(1-U)^(-1/a)
pareto = function(x){(a*(b^a)/x^(a+1))}

summary(X)

```


## Example: Pareto Distribution, pt. 3

```{r}
hist(X, probability = TRUE, breaks = 25, xlim =c(0, 20),
     col = "gray", border = "white",
     main = "Inverse Transform: Pareto(3,2)", xlab = "x")
curve(pareto(x), from = 0, to = 40, add = TRUE, col = "blue")

```






## Inverse Transform Discrete scenario
For a given an ordered discrete random sample $... < x_{i-1} < x_{i} < x_{i+1} < ...$ from a distribution $f(X)$, with CDF $F(x)$. Then, the inverse transformation $F_X^{-1}(u) = x_i$, where $F_X(x_{i-1}) < u \leq F_X(x_i)$. Then for each random variable desired,

1. Generate a random variable $u \sim Unif(0,1)$

2. Deliver $x_i$ where $F(x_{i-1}) < u \leq F(x_{i})$



As an example take the following distribution $P(X = 0) = 0.1$, $P(X = 1) = 0.2$, $P(X = 2) = 0.2$, $P(X = 3) = 0.2$, and $P(X = 4) = 0.3$, use the inverse transform method to generate a random sample of size 1000 from the distribution.

$$
F(X \leq x) = 
\begin{cases}
    0.1       & \quad  \text{if } x \leq 0 \\
    0.3       & \quad  \text{if } x \leq 1\\
    0.5       & \quad  \text{if } x \leq 2\\
    0.7       & \quad  \text{if } x \leq 3\\
    1.0       & \quad  \text{if } x \leq 4\\
  \end{cases}
$$



## Inverse Transform Discrete scenario, pt. 2


```{r}
cdf = c(0.0, 0.1, 0.3, 0.5, 0.7, 1.0)
results = numeric(length = 1000) ## creates a vector of zeros
u = runif(1000)
for(i in 2:6){
    ind = (cdf[i-1] < u) & (u <= cdf[i])
    results[ind] <- (i-2)
}

table(results) / 1000

```


## Inverse Transform Discrete scenario, pt. 3

```{r, fig.height=3, fig.width=5}

hist(results, probability = TRUE,
     main = "Inverse Transform Results",
     ylim = c(0,1),
     col = "gray", border = "white")

```




## Accept-Reject


For notes on the Accept-Rejection algorithm see [Accept-Reject](http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf)

Suppose that $X$ and $Y$ are random variables with density (or pmf) $f$ and $g$ respectively, and there exists a constant $M$ such that

$$
\frac{f(t)}{g(t)} \leq M
$$
for all t such that $f(t) > 0$. If we'd like to simulate from the target density $f(t)$, then the following algorithm can be applied to generate the random variable $X$.



## The Accept-Reject Algorithm

1. Generate $Y \sim g_Y(t)$ and $U \sim Unif(0,1)$

2. If $U \leq \frac{f(Y)}{M \times g(Y)}$ then we accept $Y$, such that $Y =  X$

3. Repeat until you have sufficient samples



In order for the algorithm to work we require the following constraings:

1. $f$ and $g$ have to have compatible supports (i.e. $g(x) > 0$ when $f(x) > 0$)

2. There is a constant $M$ such that $\frac{f(t)}{g(t)} \leq M$



## Example: Beta(2,2)

Supposed we'd like to generate 1,000 samples from $Beta(2,2)$, $f$. The density function for $Beta(2,2)$ is simply $f(x) = 6x(1 - x)$ for $0 < x < 1$. Since our domain is between 0 and 1, we can use a simple $Unif(0,1)$ density as our instrumental density, $g$. Then, by the accept-reject algorithm we can simulate a random variable $Y \sim g$, and a random variable $U \sim Unif(0,1)$. Then, if 
$$
U \leq \frac{f(Y)}{M \times g(Y)}
$$
we accept the candidate variable $Y \sim g$ as $X$, $X = Y$. Otherwise, we reject $Y$ and simulate again until we get an appropriate sample size. Note that the target density $f$ has a maximum of 1.5, so we can set M = 1.5; see: [Max of Beta(2,2)](http://www.wolframalpha.com/input/?i=max+6x(1+-+x))


```{r}
## Accept-Reject
M = 1.5
X = rep(x = NA, 5) ## create a vector of length 5 of NAs
set.seed(123)
f <- function(x){ 6*x*(1 - x)} ## pdf of Beta(2,2)
g <- function(x){ 1 } ## pdf of Unif(0,1) is just 1

n = 10000

```


## Example: Beta(2,2), pt. 2

First, we'll generate 5 samples
```{r}

for(i in 1:5){
  print(paste("Run: ", i))
  u = runif(1)
  y = runif(1)
  print(paste("U: ", u, "and Y:", y))
  accept <- u <= f(y)/(M* g(y))
  print(paste("Accept? ", accept))
  if(accept){
    X[i] <- y
  }
}

```



## Example: Beta(2,2), pt. 3


Now, say we needed $n = 10,000$ samples from $Beta(2,2)$, then a better implementation would be

```{r}
X = rep(NA, n); M = 1.5
i = 0 ## index set to start at 0
while(sum(is.na(X))){
  U = runif(1); Y = runif(1)
  accept <- U <= f(Y)/(M*g(Y))
  if(accept){
    i = i+1 ## update the index
    X[i] <- Y
  }
}

round(summary(X), 4)
round(summary(rbeta(n = n, shape1 = 2, shape2 = 2)), 4)
```


## Example: Beta(2,2), pt. 4


```{r, echo = FALSE, fig.height=3, fig.width=9}

par(mfrow = c(1,3), mar = c(3, 4, 1, 2))

## plot 1
hist(X, xlab = "X", main = "Beta(2,2) from Accept-Reject algorithm", 
     probability = TRUE)
beta <- function(x) 6*x*(1-x)
curve(expr = beta, from = 0, to = 1, add = TRUE)


## plot 2
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.5, 1.5), ylim = c(0,2),
      main = "Beta(2,2) Density with Unif(0,1)",
      xlab = "x", ylab = "density")

x = seq(from = -1, to = 2, by = 0.01)
Unif1 = function(x){ ifelse(x >= 0 & x <= 1, 1, 0) }
polygon(x, Unif1(x), lty = 9)

## plot 3
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.2, 1.2), ylim = c(0,2),
      main = "Beta(2,2) with M*Unif(0,1)",
      xlab = "x", ylab = "density")

Unif2 = function(x){ ifelse(x >= 0 & x <= 1, 1*M, 0) }
polygon(x, Unif2(x), lty = 2)

#abline(h = 1.5, col = "red")
par(mfrow=c(1,1))


```

- The success of the Accept-Reject algorithm is to choose $M$ to be as small as possible while still maintaining an envelop over the target distribution


## Example: Beta(2,2), pt. 5

Code


```{r, echo = TRUE, eval=FALSE}

## plot 1
hist(X, xlab = "X", main = "Beta(2,2) from Accept-Reject algorithm", 
     probability = TRUE)
beta <- function(x) 6*x*(1-x)
curve(expr = beta, from = 0, to = 1, add = TRUE)

## plot 2
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.5, 1.5), ylim = c(0,2),
      main = "Beta(2,2) Density with Unif(0,1)", xlab = "x", ylab = "density")

x = seq(from = -1, to = 2, by = 0.01)
Unif1 = function(x){ ifelse(x >= 0 & x <= 1, 1, 0) }
polygon(x, Unif1(x), lty = 9)

## plot 3
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.2, 1.2), ylim = c(0,2),
      main = "Beta(2,2) with M*Unif(0,1)",  xlab = "x", ylab = "density")

Unif2 = function(x){ ifelse(x >= 0 & x <= 1, 1*M, 0) }
polygon(x, Unif2(x), lty = 2)

```




## Example: Beta(2,2), pt. 6



```{r}
N = 10000; U = runif(N); Y = runif(N); M = 1.5

f <- function(x){ 6*x*(1 - x)} ## pdf of Beta(2,2)
g <- function(x){ 1 } ## pdf of Unif(0,1) is just 1

accept <- U*M < f(Y)/(g(Y))

mean(accept) ## acceptance rate
print(1/M) ## probability of acceptance

```

Exercise: Change the value of M to see how the performance of the algorithm changes

## Example: Beta(2,2), pt. 7

```{r, echo=FALSE, fig.height=4, fig.width=5}
par(mar = c(3, 4, 1, 2))

plot(Y, U*M, col = as.numeric(accept)+3, 
     xlim = c(-0.2, 1.2), ylim = c(0,2),
     main = "Accept-Reject with M = 1.5")

curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.5, 1.5), ylim = c(0,2),
      main = "Beta(2,2) with M*Unif(0,1)",
      xlab = "x", ylab = "density", add = TRUE,
      lwd = 4)

Unif2 = function(x){ ifelse(x >= 0 & x <= 1, 1*M, 0) }
polygon(x, Unif2(x), lty = 2, lwd = 2)

```


It should be noted that the probability of acceptance is given by $\frac{1}{M}$. So, in order to make an efficient accept-reject algorithm, we should set $M$ to be as high as needed, but no larger! As $M$ increases, the probability of acceptance decreases, and this results in an increase in draws where we do not obtain samples from our target distribution $f$. This increases the computational cost. 



## Example: Beta(2,2), pt. 7 | Code

```{r, echo=TRUE, eval=FALSE}
plot(Y, U*M, col = as.numeric(accept)+3, 
     xlim = c(-0.2, 1.2), ylim = c(0,2),
     main = "Accept-Reject with M = 1.5")

curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.5, 1.5), ylim = c(0,2),
      main = "Beta(2,2) with M*Unif(0,1)",
      xlab = "x", ylab = "density", add = TRUE,
      lwd = 4)

Unif2 = function(x){ ifelse(x >= 0 & x <= 1, 1*M, 0) }
polygon(x, Unif2(x), lty = 2, lwd = 2)

```




## Transformation Methods

- Distributions sometimes share relationships, and if the relationship is simple we can exploit it to generate random variables from a "simple to simulate" distribution and transform those random variables to generate samples from a "harder to simulate" distribution. I'll give some examples below. 

- First, see the following Wikipedia page for some relationships: [Relationships among probability distributions][4]

Now that we've covered how to simulate random variables from an $Exp(\theta)$ distribution ("simple to simulate"), I'll cover how to generate some random variables from "harder to simulate" distributions.

We'll cover the following:

1. $Y = \sum^N_{i=1} X$ where $Y \sim \chi^2_{2N}$ ($2N$ degress of freedom). See: [Chi-Square distribution][5]

2. $Y = \beta \sum^\alpha_{i = 1} X$ where $Y \sim Gamma(\alpha, \beta)$. See: [Gamma distribution][6]

3. $Y = \frac{\sum^a_{i = 1} X}{\sum^{a + b}_{i = 1} X}$ where $Y \sim Beta(a, b)$. See: [Beta distribution][7]

4. $Y = \sum_{i = 0}^{N} X_i$ where $X ~ Bernoulli(p)$ such that $Y \sim Bin(n,p)$


## Example: Binomial trials

```{r}
## let's simulate 10,000 samples from bin(n=10, p = 0.3)
N = 10000
n = 10
p = 0.3
x = as.numeric(runif(n = (n*N)) <= p) ## convert bools to 1 and 0
m = matrix(data = x, nrow = N, ncol = n)
Binom_samples = rowSums(m) 
```


## Example: Binomial trials, pt. 2

```{r}
par(mfrow = c(1,2))
hist(Binom_samples, probability = TRUE, main = "Binom(10,0.3) from Unif(0,1)", 
     col = "blue")
hist(rbinom(n = N, size = n, prob = p), probability = TRUE, main = "rbinom(10,0.3)",
     col = "cyan")
par(mfrow=c(1,1))



```



## Example: Generate Rayleigh samples

First attempt the Inverse Transform Method and see why it won't work.

For information on the Rayleigh distribution follow the link: [Rayleigh Distribution][2]

CDF: $F(X \leq x) = 1 - exp(\frac{-x^2}{2\sigma^2})$


Inverse Transform:
Set $F(x) = U$, where $U \sim Unif(0,1)$.

$$
\displaystyle
\begin{aligned}
1 - exp \left(\frac{-x^2}{2\sigma^2} \right) & =  U \\
\ exp \left(\frac{-x^2}{2\sigma^2} \right) & = 1 - U \\
\ log \left(exp \left(\frac{-x^2}{2\sigma^2} \right) \right) & = log(1 - U) \\
\ \frac{-x^2}{2\sigma^2} & = log(1 - U)  \\
\ -x^2 & = 2\sigma^2 \times log(1 - U)  \\
\ x & = \sqrt{ - 2\sigma^2 \times log(1 - U) } \\
\end{aligned}
$$

- can't take the square root of a negative value


## Example: Generate Rayleigh samples

From the last equation, we see that we'd be taking the square root of negative values which would be problematic. Therefore, we need an alternative algorithm. 

From information on the Rayleigh distribution, we know that given two i.i.d. random variables $Z_1, Z_2 \sim N(0, \sigma)$ then $R = \sqrt{ Z_1^2 + Z_2^2 } \sim Rayleigh(\sigma)$. Therefore, in order to simulate 1 random variable from $Rayleigh(\sigma)$, we first generate 2 random variables from a Normal distribution with mean 0 and standard deviation $\sigma$.

To generate $N$ Rayleigh random variables, our algorithm would be:

1. Generate $2 \times N$ random variables $Z_i \sim N(0,\sigma)$ for $i \in (0, 2N)$
2. For each pair of $Z_i \sim N(0,\sigma)$ use the transformation $R = \sqrt{ Z_1^2 + Z_2^2 }$ to obtain $N$ random variables from $Rayleigh(\sigma)$.


## Example: Generate Rayleigh samples

```{r}
N = 10000
Z = rnorm(n = 2*N, mean = 0, sd = 1)
Z = matrix(data = Z, nrow = N, ncol = 2)

transfromation <- function(vec){
  R = sqrt(sum(vec^2))
  #R = sqrt(vec[1]^2 + vec[2]^2)
  return(R)
}

R_Out = apply(X = Z, MARGIN = 1, FUN = transfromation)

summary(R_Out)
sqrt(pi/2) ## theoretical mean

```




## Example: Generate Rayleigh samples, pt.2

```{r, fig.height=3, fig.width=5}

hist(R_Out, col = "gray", border = "white")
## compare with Rayleigh {VGAM}	

```












[1]: https://stats.stackexchange.com/questions/184325/how-does-the-inverse-transform-method-work
[2]: http://www.math.uah.edu/stat/special/Rayleigh.html
[3]: http://www.math.uah.edu/stat/special/Pareto.html
[4]: https://en.wikipedia.org/wiki/Relationships_among_probability_distributions
[5]: http://www.math.uah.edu/stat/special/ChiSquare.html
[6]: http://www.math.uah.edu/stat/special/Gamma.html
[7]: http://www.math.uah.edu/stat/special/Beta.html


