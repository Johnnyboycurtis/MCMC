{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now importing: \n",
      "1. numpy as np \n",
      "2. matplotlib.pyplot as plt\n",
      "3. from scipy stats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prereqs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM Algorithm  \n",
    "\n",
    "\n",
    "Given a random sample of size $n$, with observed sample $\\mathbf{X} = (X_1, ..., X_m)$ and *missing* random sample $\\mathbf{Z} = Z_{m+1}, ..., Z_n$ we seek to compute \n",
    "$$\n",
    "\\hat{\\theta} = \\text{ arg max } L(\\theta | \\mathbf{X}, \\mathbf{Z}) \n",
    "$$\n",
    "Although $\\mathbf{Z}$ is unobservable, we assume that $(\\bf{X, Z}) \\sim f(\\bf{x,z} | \\theta)$.\n",
    "\n",
    "We place a conditional distribion on $\\mathbf{Z}$ given the observed data $\\bf{x}$,\n",
    "$$\n",
    "k(\\mathbf{z}| \\theta, \\mathbf{x}) = f( \\mathbf{x, z} | \\theta) / g(\\mathbf{x} | \\theta)\n",
    "$$\n",
    "\n",
    "Here we assume that that $\\mathbf{X} \\sim g(\\bf{x} | \\theta)$, where \n",
    "$$\n",
    "g(\\bf{x} | \\theta) = \\int f( \\mathbf{x,z} | \\theta ) d\\bf{z}\n",
    "$$\n",
    "\n",
    "\n",
    "## The EM Algorithm  \n",
    "\n",
    "Denote the complete-data likelihood as $L^c(\\theta | \\mathbf{x, z})$ and the observed-data likelihood as $L(\\theta | \\mathbf{x} )$. Then, for any value of $\\theta$, $\\theta_i$\n",
    "\n",
    "$$\n",
    "log L(\\theta | \\mathbf{x}) = E[ log L^c(\\theta | \\mathbf{x, z}) ] - E [ log k( \\bf{Z} | \\theta_i, \\bf{x} ) ]\n",
    "$$\n",
    "where the expectation is with respect to $k(\\mathbf{z}| \\theta_i, \\bf{x})$. We can rewrite this as\n",
    "\n",
    "\n",
    "$$\n",
    "E[log L^c(\\theta | \\mathbf{x, z})] = log L(\\theta | \\mathbf{x}) +  E[log k(\\mathbf{Z} | \\theta_i, \\bf{x})]\n",
    "$$\n",
    "\n",
    "where our focus is concerned with maximizing $E[log L^c(\\theta | \\mathbf{x, z})]$.\n",
    "\n",
    "\n",
    "\n",
    "## The EM Algorithm  \n",
    "\n",
    "Denoting $E[log L^c(\\theta | \\mathbf{x, z})]$ = $Q(\\theta | \\theta_i, \\mathbf{x})$, the EM algorith iterates through values of $\\theta_i$ by maximizing $Q(\\theta | \\theta_i, \\mathbf{x})$.\n",
    "\n",
    "**The EM Algorithm**\n",
    "\n",
    "`Pick a starting value` $\\hat{\\theta_0}$\n",
    "\n",
    "`Then for i in 1:n do`\n",
    "\n",
    "`1. Compute` (E-step)\n",
    "$$\n",
    "Q(\\theta | \\theta_{i-1}, \\mathbf{x}) = E[log L^c(\\theta | \\mathbf{x, z})]\n",
    "$$\n",
    "\n",
    "`where the expectation is with respect to` $k( \\bf{Z} | \\theta_i, \\bf{x} )$\n",
    "\n",
    "`2. Maximize` $Q(\\theta | \\theta_{i-1}, \\mathbf{x})$ `in` $\\theta$ `and take`\n",
    "\n",
    "$$\n",
    "\\hat{\\theta_i} = \\text{ arg max } Q(\\theta | \\theta_{i-1}, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "`repeat until convergence criteria is met`\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "This exercise is taken from Flury and Zoppe, 2000, see [Exercises in EM](http://www.webpages.uidaho.edu/~stevel/565/literature/Exercise%20in%20EM.pdf).\n",
    "\n",
    "Below is the setup for the first exercise.\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "There are two light bulb survival experiments. \n",
    "\n",
    "In the first, there are $N$ bulbs,  $y_1, ..., y_N$,  whose exact lifetimes are recorded. The lifetimes have an exponential distribution, such that $y_i \\sim Exp(\\theta).$ \n",
    "\n",
    "In the second experiment, there are $M$ bulbs, $x_1, ..., x_M$. After some time *t* > 0, a researcher walks into the room and only records how many lightbulbs are still burning out of $M$ bulbs. Depending on whether the lightbulbs are still burning or out, the results from the second experiment are right- or -left-censored. There are indicators $E_1, ..., E_M$ for each of the bulbs in the second experiment. If the bulb is still burning, $E_i = 1$, else $E_i = 0$.\n",
    "\n",
    "Given this information, our task is to solve for an MLE estimator for $\\theta$.\n",
    "\n",
    "Our first step in solving this is finding the joint likelihood for the observed and unobserved data (i.e. complete-data likelihood).\n",
    "\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "Let $X_1, ... , X_M$ be the (unobserved) lifetimes for the second experiment, and let $Z = \\sum_{i=1}^ME_i$ be the number of light bulbs still burning. Thus, the observed data from both the experiments combined is $\\mathcal{Y} = (Y_1, ..., Y_N, E_1,...,E_M)$ and the unobserserved data is $\\mathcal{X} = (X_1, ..., X_M).$\n",
    "\n",
    "> The complete data log-likelihood is obtained by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\displaystyle L(\\theta| X,Y) & = \\prod^N_{i=1} \\frac{1}{\\theta} e^{y_i/\\theta} \\times \\prod^M_{i=1} \\frac{1}{\\theta} e^{x_i/\\theta}   \\\\\n",
    "\\ & = \\displaystyle \\theta^{-N} e^{-N \\bar{y} / \\theta} \\times \\theta^{-M} e^{-\\sum^M_{i=1} x_i / \\theta }  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "And log-likelihood is obtained by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "log(L(\\theta)) & = \\displaystyle -N \\times log(\\theta) - N \\bar{y}/\\theta - M \\times log(\\theta) + \\sum^M_{i=1} x_i / \\theta  \\\\\n",
    "\\ & = \\displaystyle -N ( log(\\theta) + \\bar{y}/\\theta ) - M \\times log(\\theta) + \\sum^M_{i=1} x_i / \\theta  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Or as written by Flury and Zoppe,\n",
    "\n",
    "$$\n",
    "log^c(L(\\theta|\\mathcal{Y,X})) = -N(log(\\theta) + \\bar{Y}/\\theta) - \\sum_{i=1}^M(log(\\theta) + X_i/\\theta ) \\tag{1}\n",
    "$$ \n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "The next step, is to take the expectation of $log(L(\\theta))$ with respect to observed data.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[log(L(\\theta)) | \\mathcal{Y},\\mathcal{X}] & = E[-N(log(\\theta) + \\bar{Y}/\\theta) - \\sum_{i=1}^M(log(\\theta) + X_i/\\theta ) | \\mathcal{Y},\\mathcal{X}] \\\\\n",
    "\\ & = -N(log(\\theta) + \\bar{Y}/\\theta) - E[\\sum_{i=1}^M(log(\\theta) + X_i/\\theta ) | \\mathcal{Y},\\mathcal{X}]   \\\\\n",
    "\\ & = -N(log(\\theta) + \\bar{Y}/\\theta) - M \\times log(\\theta) + E[\\frac{1}{\\theta}  \\sum_{i=1}^M X_i | \\mathcal{Y},\\mathcal{X}]   \\\\\n",
    "\\ & = -N(log(\\theta) + \\bar{Y}/\\theta) - M \\times log(\\theta) + \\frac{1}{\\theta} \\sum_{i=1}^M E[X_i | \\mathcal{Y},\\mathcal{X}]   \\\\\n",
    "\\ & = -N(log(\\theta) + \\bar{Y}/\\theta) - M \\times log(\\theta) + \\frac{1}{\\theta} \\sum_{i=1}^M E[X_i | E_i]   \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "> which is linear for unobserved $X_i$. But\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "$$\n",
    "E[X_i | \\mathcal{Y}] = E[X_i | E_i] = \n",
    "\\begin{cases}\n",
    "    t + \\theta       & \\quad \\text{if } E_i = 1\\\\\n",
    "    \\theta - t \\frac{e^{-t/\\theta}}{1 - e^{-t/\\theta}}  & \\quad \\text{if } E_i = 0  \\tag{2} \\\\\n",
    "  \\end{cases} \n",
    "$$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "For the first case, $E_i = 1$, so\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[x_i | x_i > t] & = E[x_i + t] \\\\\n",
    "\\ & = t + E[x_i] \\\\\n",
    "\\ & = t + \\theta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For the second case, $E_i = 0$, then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\displaystyle \\int_0^t P(X_i > x | X_i < t) \\ dx & = \\int_0^t \\frac{P(x < X_i < t)}{P(X_i < t)} \\ dx \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "For the denominator, we get\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X_i < t) & = \\int_0^t \\frac{1}{\\theta} e^{- x_i / \\theta} dx \\\\\n",
    "\\ & = \\frac{1}{\\theta} (-\\theta e^{- x_i / \\theta}) |^t_0 \\\\\n",
    "\\ & = 1 - e^{- t / \\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and for the numerator we obtain\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x < X_i < t) & = \\int_x^t \\frac{1}{\\theta} e^{- x_i / \\theta} dx \\\\\n",
    "\\ & = \\frac{1}{\\theta} (-\\theta e^{- x_i / \\theta}) |^t_0 \\\\\n",
    "\\ & = e^{- x / \\theta} - e^{- t / \\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "\n",
    "Altogether, we obtain\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\displaystyle \\int_0^t P(X_i > x | X_i < t) \\ dx & = \\int_0^t \\frac{P(x < X_i < t)}{P(X_i < t)} \\ dx \\\\\n",
    "\\ & = \\displaystyle \\int_0^t \\frac{e^{- x / \\theta} - e^{- t / \\theta}}{(1 - e^{-t/\\theta})} \\ dx \\\\\n",
    "\\ & = \\displaystyle \\frac{1}{(1 - e^{-t/\\theta})} \\int_0^t (e^{- x / \\theta} - e^{- t / \\theta}) \\ dx \\\\\n",
    "\\ & = \\displaystyle \\frac{1}{(1 - e^{-t/\\theta})}  (\\int_0^t e^{- x / \\theta} - \\int_0^te^{- t / \\theta} \\ dx) \\\\\n",
    "\\ & = \\displaystyle \\frac{1}{(1 - e^{-t/\\theta})}  (\\theta (1 - e^{-t/\\theta}) - x\\times e^{- t / \\theta} |_0^t) \\\\\n",
    "\\ & = \\displaystyle   \\theta - t\\times \\frac{e^{- t / \\theta}}{1 - e^{-t/\\theta}}  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "In order to calculate EM esimates for $\\theta$, we will plug in the expected values into the log-likelihood\n",
    "\n",
    "$$\n",
    "E[X_i | \\mathcal{Y}] = E[X_i | E_i] = \n",
    "\\begin{cases}\n",
    "    t + \\theta       & \\quad \\text{if } E_i = 1\\\\\n",
    "    \\theta - t \\frac{e^{-t/\\theta}}{1 - e^{-t/\\theta}}  & \\quad \\text{if } E_i = 0\\\\\n",
    "  \\end{cases} \n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(L(\\theta)) & = \\displaystyle -N ( log(\\theta) + \\bar{y}/\\theta ) - M \\times log(\\theta) + \\sum^M_{i=1} x_i / \\theta  \\\\\n",
    "\\ & = \\displaystyle -N \\times log(\\theta) - N \\bar{y}/\\theta  - M \\times log(\\theta) + \\sum^M_{i=1} x_i / \\theta  \\\\\n",
    "\\ & = \\displaystyle -( N + M)  \\times log(\\theta) - N \\bar{y}/\\theta + \\sum^M_{i=1} x_i / \\theta  \\\\\n",
    "\\ & = \\displaystyle -( N + M)  \\times log(\\theta) - \\frac{1}{\\theta} (N \\bar{y} + \\sum^M_{i=1} x_i )  \\\\\n",
    "\\ & = -(N + M) log(\\theta) - \\frac{1}{\\theta} \\big[N \\bar{Y} + Z ( t + \\theta) + (M - Z) \\big(\\theta - t \\times \\frac{e^{- t / \\theta}}{1 - e^{-t/\\theta}} \\big)\\big] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "As we iterate through estimates of $\\theta$, we will use conditioned estimates of $\\theta$ given previous estimates of $\\theta$. Such that the $j$th step consists of replacing $X_i$ in (1) by its expected value (2), using the current numerical parameter value $\\theta^{(j-1)}$. \n",
    "\n",
    "$$\n",
    "\\log(L(\\theta)) = -(N + M) log(\\theta) - \\frac{1}{\\theta} [N \\bar{Y} + Z ( t + \\theta^{(j-1)}) + (M - Z) (\\theta^{(j-1)} - t p^{(j-1)})] \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "> where \n",
    "\n",
    "$$\n",
    "p^{(j-1)} = \\frac{e^{-t/\\theta^{(j-1)}}}{1 - e^{-t/\\theta^{(j-1)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "Once we take the derivative of the log-likelihood and set it to zero, we will come up with an estimate for $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\displaystyle\n",
    "\\frac{\\mathrm d}{\\mathrm d x} ln(L(\\theta)) & = 0 \\\\\n",
    "\\ \\displaystyle 0 & = -\\frac{(N + M)}{\\theta} + \\frac{1}{\\theta^2} \\big[N \\bar{Y} + Z ( t + \\theta^{(j-1)}) + (M - Z) \\big(\\theta^{(j-1)} - t \\times \\frac{e^{- t / \\theta^{(j-1)}}}{1 - e^{-t/\\theta^{(j-1)}}} \\big)\\big]  \\\\\n",
    "\\ \\displaystyle \\frac{(N + M)}{\\theta} & =  \\frac{1}{\\theta^2} \\big[N \\bar{Y} + Z ( t + \\theta^{(j-1)}) + (M - Z) \\big(\\theta^{(j-1)} - t \\times \\frac{e^{- t / \\theta^{(j-1)}}}{1 - e^{-t/\\theta^{(j-1)}}} \\big) \\big] \\\\\n",
    "\\ \\displaystyle \\theta & =  \\big[N \\bar{Y} + Z ( t + \\theta^{(j-1)}) + (M - Z) \\big(\\theta^{(j-1)} - t \\times \\frac{e^{- t / \\theta^{(j-1)}}}{1 - e^{-t/\\theta^{(j-1)}}} \\big)\\big] \\ / \\ (N+M) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## The First Exercise  \n",
    "\n",
    "\n",
    "Thus, for each *j*th M-step, we will calculate\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^{(j)} & = f(\\theta^{(j-1)}) \\\\\n",
    "\\ \\displaystyle \\theta & =  \\big[N \\bar{Y} + Z ( t + \\theta^{(j-1)}) + (M - Z) \\big(\\theta^{(j-1)} - t \\times \\frac{e^{- t / \\theta^{(j-1)}}}{1 - e^{-t/\\theta^{(j-1)}}} \\big)\\big] \\ / \\ (N+M) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5678)\n",
    "theta = 5 ## theta\n",
    "\n",
    "t = 5 ## time cut off\n",
    "N = 100 ## sample size of ex 1\n",
    "M = 50 ## sample size of ex 2\n",
    "y = np.random.exponential(size=N, scale = theta) ## first experiment\n",
    "x = np.random.exponential(size=M, scale = theta) ## second experiment\n",
    "x = np.sort(x)\n",
    "E = np.int64(x > t)  ## 0 & 1\n",
    "\n",
    "ybar = np.mean(y)\n",
    "Z = np.sum(E)\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.066172364533033"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.17740132763\n",
      "1 4.95589783696\n",
      "2 5.06975690426\n",
      "3 5.08603680591\n",
      "4 5.08835807946\n",
      "5 5.08868893005\n",
      "6 5.08873608351\n",
      "7 5.08874280385\n",
      "8 5.08874376164\n",
      "9 5.08874389814\n"
     ]
    }
   ],
   "source": [
    "theta_j = 0.1\n",
    "theta_jp1 = 0.5\n",
    "for i in range(10):\n",
    "    theta_j = theta_jp1\n",
    "    p = (np.exp(-t/theta_j)/(1 - np.exp(-t/theta_j)))\n",
    "    theta_jp1 = (N*ybar + Z*( t + theta_j) + (M-Z)*(theta_j - t*p) ) / (N+M)\n",
    "    print(i, theta_jp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.08874389814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.127762652795429"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compare results\n",
    "print(theta_jp1) ## EM theta estimate\n",
    "np.mean(y) ## compare against MLE from observed data\n",
    "np.mean(np.concatenate((y, x))) ## compare against complete-data\n",
    "## note, results will vary if you remove seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Normal Example  \n",
    "\n",
    "Suppose $X = (x_1, ..., x_n)^T$ is a random sample from $N(\\mu,1).$ Let the observations be in order such that $x_1 < x_2 < ... < x_n$. \n",
    "Suppose that after time $c$, values are censored or missing, such that only $x_1, ..., x_m$ are observed, and $x_{m+1}, ..., x_n$ are unobserved. \n",
    "Then, $r = (n - m)$ would be the quantity missing. We will use the EM and MCEM algorithms to find approximations for $\\mu$.\n",
    "Let $Z = (x_{m+1}, ..., x_n)^T$.\n",
    "\n",
    "First, construct the likelihood function.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mu | x) & = \\prod^m f(x_i | \\mu, 1) \\times \\prod^r f(z_i | \\mu, 1) \\\\\n",
    "\\ & = (2 \\pi )^{-n/2} exp(-\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) \\times exp(-\\frac{1}{2} \\sum_{i=1}^m (z_i - \\mu)^2) \\\\\n",
    "\\ & \\propto exp(-\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) \\times exp(-\\frac{1}{2} \\sum_{i=1}^m (z_i - \\mu)^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## EM Normal Example  \n",
    "\n",
    "The log-likelihood is then \n",
    "\n",
    "$$\n",
    "ln(L(\\mu | X)) = -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) - \\frac{1}{2} \\sum_{i=1}^m (z_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "We now find the conditional expectation $E[z_i | X]$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[z_i | X] & =  E[z_i | x > c] = \\int_c^{\\infty} \\frac{P(x_i > x | x_i > c)}{P(x_i > c)}   \\\\\n",
    "\\ & = \\mu + \\sigma \\frac{\\phi(c - \\mu)}{1 - \\Phi(c - \\mu)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For notes on this derivation, see [Truncated Normal Distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution#Moments)\n",
    "\n",
    "\n",
    "\n",
    "## EM Normal Example  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(\\mu | \\mu_t) & = -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) - \\sum E[z_i | X] \\\\\n",
    "\\ & =   -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) - \\sum E[z | X] \\\\\n",
    "\\ & =   -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^2) - (n-m) E[z | X] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The MLE for $\\mu$ is then,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{t+1} & =  \\frac{m \\bar{x}}{n} + \\frac{(n - m) E[z | X]}{n} \\\\\n",
    "\\ & = \\frac{m \\bar{x}}{n} + \\frac{(n - m) (\\mu_t)}{n} + \\frac{(n-m) \\phi(c - \\mu_t)}{n \\Phi(c-\\mu_t)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2345)\n",
    "n = 100\n",
    "mu = 4\n",
    "sd = 1\n",
    "x = np.random.normal(size=n, loc=mu, scale=sd) ## generate some data\n",
    "c = 5 ## time cut off\n",
    "w = x[x < c] ## obtain samples before time cut off\n",
    "m = np.sum(x < c) ## number of observed samples\n",
    "wbar = np.mean(w) ## observed mean\n",
    "r = n - m ## difference in sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0023998029\n"
     ]
    }
   ],
   "source": [
    "## EM Normal Example  \n",
    "from scipy import stats\n",
    "dnorm = stats.norm.pdf ## get density values\n",
    "pnorm = stats.norm.cdf ## get tail probabilities\n",
    "\n",
    "N = 200\n",
    "mu_new = wbar\n",
    "results = []\n",
    "for i in range(N):\n",
    "    results.append(mu_new)\n",
    "    mu_old = mu_new\n",
    "    mu_new = m*wbar/n + (r*mu_old/n) + (r/n)*sd*(dnorm(c - mu_old))/(1 - pnorm(c - mu_old))  ## r/n instead of 1/n\n",
    "\n",
    "print(results[-1]) ## last value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## EM Normal Example  \n",
    "\n",
    "```{r}\n",
    "plot(results, type = \"l\", main = \"em estimates for mu\", ylim = c(3.5, 4.5))\n",
    "abline(h = mu, col = \"red\")\n",
    "abline(h = wbar, col = \"green\", lty = 2)\n",
    "abline(h = mean(x), col = \"blue\", lty = 3)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Monte Carlo EM \n",
    "\n",
    "A MC flavor of the EM algorithm\n",
    "\n",
    "1. Draw missing data sets $\\mathbf{Z_1, Z_2, ..., Z_m} \\sim f_{Z|X}(z | x, \\theta_i)$ where each $\\mathbf{Z_i}$ is a vector of all missing values needed to complete the observed data set $( \\mathbf{X, Z} )$.\n",
    "\n",
    "2. Calculate $\\bar{Q}(\\theta | \\theta_{i-1}, X, \\mathbf{Z_1, ..., Z_m}) = \\frac{1}{m} \\sum_{i=1}^m Q(\\theta | \\theta_{i-1}, X, \\mathbf{Z_i} )$\n",
    "\n",
    "\n",
    "\n",
    "## EM Normal Example | Monte Carlo EM \n",
    "\n",
    "\n",
    "```{r}\n",
    "\n",
    "set.seed(2345)\n",
    "n = 100\n",
    "mu = 4\n",
    "sd = 1\n",
    "x = rnorm(n, mu, sd)\n",
    "c = 5\n",
    "w = x[x < c]\n",
    "m = sum(x < c)\n",
    "wbar = mean(w)\n",
    "r = n - m\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## EM Normal Example| Monte Carlo EM   \n",
    "\n",
    "\n",
    "```{r}\n",
    "M = 10\n",
    "N = 100\n",
    "mu_new = wbar\n",
    "results = numeric(N)\n",
    "for(i in 1:N){\n",
    "    results[i] = mu_new\n",
    "    mu_old = mu_new\n",
    "    ## abs(N(0,1)) + mu_old + (c - mu_old) to *approximate*\n",
    "    ## the truncated samples we need\n",
    "    Z = matrix(data = (c - mu_old) + (mu_old +  abs(rnorm(n = r*M, mean = 0, sd = 1))), \n",
    "        nrow = r, ncol = M)\n",
    "    mu_new = (m*wbar/n) + mean(colMeans(Z))*r/n\n",
    "    M = M + 1\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## EM Normal Example| Monte Carlo EM   \n",
    "\n",
    "\n",
    "```{r, fig.height=3.5}\n",
    "plot(results, type = \"l\", ylim = c(3.5, 4.5))\n",
    "abline(h = mu, col = \"red\")\n",
    "abline(h = wbar, col = \"green\", lty = 2)\n",
    "abline(h = mean(x), col = \"blue\", lty = 3)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
